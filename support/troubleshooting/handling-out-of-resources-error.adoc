[id="admin-guide-handling-out-of-resource-errors"]
= Handling Out of Resource Errors
include::modules/common-attributes.adoc[]

toc::[]

== Overview

This topic discusses best-effort attempts to prevent {product-title} from experiencing out-of-memory (OOM) and out-of-disk-space conditions.

Having a stable node when it's available compute resources are low is integral to the smooth functioning of your cluster. This is especially important when dealing with incompressible resources, such as memory or disk; if either resource is exhausted, the node becomes unstable.

As an administrator, you can proactively monitor nodes and prevent them from running out of compute and memory resources using configurable xref:out-of-resource-eviction-policy[eviction policies].

This topic also provides information on how {product-title} handles out-of-resource conditions and provides an xref:out-of-resource-schedulable-resources-and-eviction-policies[example scenario].


[WARNING]
====
If you enable swap memory for a node, that node cannot detect that it is under memory pressure.

To take advantage of memory-based evictions, you must xref:../admin_guide/overcommit.adoc#disabling-swap-memory[disable swap].
====



[[out-of-resource-create-config]]
== Configuring Eviction Policies

An _eviction policy_ allows a node to fail one or more pods when it is running low on available resources. Failing a pod allows the node to reclaim needed resources.

An eviction policy is a set of xref:out-of-resource-eviction-thresholds[eviction thresholds] that you can specify in the node configuration file or through the xref:out-of-resource-eviction-thresholds[command line].
//an xref:out-of-resource-eviction-signals[eviction trigger signal] and a specific xref:out-of-resource-eviction-thresholds[eviction threshold value]


Well-configured eviction policies help to prevent total consumption of a compute resource on a node.

.Procedure

. To configure an eviction policy, open the appropriate xref:../admin_guide/manage_nodes.adoc#modifying-nodes[node configuration map] and add a set of xref:out-of-resource-eviction-thresholds[eviction thresholds].
+
[NOTE]
====
To modify a node in your cluster, update the xref:../admin_guide/manage_nodes.adoc#modifying-nodes[node configuration maps] as needed. Do not manually edit the `node-config.yaml` file.
====
+
An eviction threshold consists of an xref:out-of-resource-eviction-signals[eviction signal], an operator, and a quantity in the following format:
+
----
<eviction_signal><operator><quantity>
----
+
* The `eviction-signal` value can be any xref:out-of-resource-eviction-signals-supported[supported eviction signal].
* The `operator` value is `<`.
* The `quantity` value must match the link:https://github.com/kubernetes/community/blob/master/contributors/design-proposals/scheduling/resources.md#resource-quantities[quantity representation] used by
Kubernetes and can be expressed as a percentage if it ends with the `%` token.
+
For example, if you have a node with 10Gi of memory, and you want to induce eviction when the available memory falls below 1Gi, you can specify an eviction threshold for memory as either of the following:
+
[source,yaml]
----
memory.available<1Gi
memory.available<10%
----
+
[[out-of-resource-eviction-monitoring-interval]]
[NOTE]
====
A node evaluates and monitors eviction thresholds every 10 seconds and the value can not be modified. This is the housekeeping interval.
====
+
An eviction threshold can be either xref:out-of-resource-hard-eviction-thresholds[hard], where a node takes immediate action on a pod that exceeds a threshold, or xref:out-of-resource-soft-eviction-thresholds[soft], where a node allows a grace period before taking action.
+
To set a hard eviction threshold, add the threshold under the `eviction-hard` parameter and to set a soft eviction threshold, add it under the `eviction-soft` parameter.
+
The following samples show various eviction thresholds that you can set:
+
.Sample Node Configuration File for a Hard Eviction
[source,yaml]
----
kubeletArguments:
  eviction-hard: <1>
  - memory.available<100Mi <2>
  - nodefs.available<10%
  - nodefs.inodesFree<5%
  - imagefs.available<15%
  - imagefs.inodesFree<10%
----
<1> The type of eviction: Use this parameter for a xref:out-of-resource-hard-eviction-thresholds[hard eviction].
<2> Eviction thresholds based on a specific eviction trigger signal.
+
[NOTE]
====
You must provide percentage values for the `inodesFree` parameters. You can provide percentage or numerical values for the other parameters.
====
+
.Sample Node Configuration File for a Soft Eviction
[source,yaml]
----
kubeletArguments:
  eviction-soft: <1>
  - memory.available<100Mi <2>
  - nodefs.available<10%
  - nodefs.inodesFree<5%
  - imagefs.available<15%
  - imagefs.inodesFree<10%
  eviction-soft-grace-period:<3>
  - memory.available=1m30s
  - nodefs.available=1m30s
  - nodefs.inodesFree=1m30s
  - imagefs.available=1m30s
  - imagefs.inodesFree=1m30s
----
<1> The type of eviction: Use this parameter for a xref:out-of-resource-hard-eviction-thresholds[soft eviction].
<2> An eviction threshold based on a specific eviction trigger signal.
<3> The grace period for the soft eviction. For optimal performance, leave the default values as they are.

. Restart the {product-title} service for the changes to reflect:
+
ifdef::openshift-enterprise[]
[source,terminal]
----
# systemctl restart atomic-openshift-node
----
endif::[]
ifdef::openshift-origin[]
[source,terminal]
----
# systemctl restart origin-node
----
endif::[]


To know how pod eviction works after you have successfully configured an eviction policy for a node, see xref:out-of-resource-eviction-of-pods[Understanding Pod Eviction].

[[out-of-resource-eviction-signals]]
=== Understanding Eviction Signals

Eviction signals indicate out-of-memory (OOM) or out-of-disk-space conditions. You can configure a node to trigger eviction decisions on an eviction signal.

To view the supported signals, run the following command:

[source,terminal]
----
curl <certificate details> \
  https://<master>/api/v1/nodes/<node>/proxy/stats/summary
----

////
curl --cacert /path/to/ca.crt
////
The following table lists all the supported eviction signals.

[[out-of-resource-eviction-signals-supported]]
.Supported Eviction Signals
[cols="2a,2a,2a,10a",options="header"]
|===

|Node Condition |Eviction Signal | Value |Description

|`MemoryPressure`
|`memory.available`
|`memory.available` = `node.status.capacity[memory]` - `node.stats.memory.workingSet`
| Available memory on the node has exceeded an eviction threshold.

.4+|`DiskPressure`
|`nodefs.available`
|`nodefs.available` = `node.stats.fs.available`
.4+| Available disk space on either the node root file system or the image file system has exceeded an eviction threshold.

|`nodefs.inodesFree`
|`nodefs.inodesFree` = `node.stats.fs.inodesFree`

|`imagefs.available`
|`imagefs.available` = `node.stats.runtime.imagefs.available`

|`imagefs.inodesFree`
|`imagefs.inodesFree` = `node.stats.runtime.imagefs.inodesFree`
|===

Each of the signals in the preceding table supports either a literal value or a percentage, except `inodesFree`. You must specify a percentage value for the `inodesFree` signal. The percentage is calculated relative to the total capacity associated with each signal.

A script derives the value for `memory.available` from your cgroup driver using the same set of steps that the kubelet performs. The script excludes inactive file memory (that is, the number of bytes of file-backed memory on an inactive LRU list) from its calculation as it assumes that the inactive file memory is reclaimable under pressure.

[NOTE]
====
Do not use tools like `free -m`, because `free -m` does not work in a container.
====

{product-title} monitors these file systems every 10 seconds.

If you store volumes and logs in a dedicated file system, the node does not monitor that file system.

[NOTE]
====
A node supports the ability to trigger eviction decisions based on disk pressure. Before evicting pods because of disk pressure, it also performs xref:../admin_guide/garbage_collection.adoc#admin-guide-garbage-collection[container and image garbage collection].
====

[[out-of-resource-eviction-thresholds]]
=== Understanding Eviction Thresholds

You can configure eviction thresholds for a node in the xref:out-of-resource-eviction-policy[node configuration file]. Reaching a threshold triggers the node to reclaim resources.

If an eviction threshold is met, independent of its associated grace period, the node reports a condition to indicate that the node is under memory or disk pressure. Reporting the pressure prevents the scheduler from scheduling any additional pods on the node while attempts to reclaim resources are made.

A node continues to report node status updates at the frequency specified by the `node-status-update-frequency` argument. The default frequency is 10 seconds.

[NOTE]
====
When a node fails a pod, it ends all the containers in the pod, and the link:https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-phase[`PodPhase`] is transitioned to *Failed*.
====

[[out-of-resource-hard-eviction-thresholds]]
==== Hard Eviction Thresholds

A hard eviction threshold has no grace period. When a hard eviction threshold is met, the node takes immediate action to reclaim the associated resourceâ€”it can end one or more pods immediately with no graceful termination.

To configure hard eviction thresholds, add eviction thresholds to the xref:out-of-resource-eviction-policy[node configuration file] under `eviction-hard`, as shown in xref:out-of-resource-create-config[Configuring Eviction Policies].

.Sample Node Configuration File with Hard Eviction Thresholds
[source,yaml]
----
kubeletArguments:
  eviction-hard:
  - memory.available<500Mi
  - nodefs.available<500Mi
  - nodefs.inodesFree<5%
  - imagefs.available<100Mi
  - imagefs.inodesFree<10%
----
[NOTE]
====
The values in the sample file are only indicative.
====

[[out-of-resource-hard-eviction-thresholds-default]]
===== Default Hard Eviction Thresholds

{product-title} uses the following default configuration for `eviction-hard`.

[source,yaml]
----
...
kubeletArguments:
  eviction-hard:
  - memory.available<100Mi
  - nodefs.available<10%
  - nodefs.inodesFree<5%
  - imagefs.available<15%
...
----

[[out-of-resource-soft-eviction-thresholds]]
==== Soft Eviction Thresholds

A soft eviction threshold pairs an eviction threshold with a required administrator-specified grace period. The node does not reclaim resources associated with the eviction signal until that grace period is exceeded. If you do not specify a grace period in the node configuration, the node produces an error on startup.

[NOTE]
====
Soft eviction is commonly used to target a certain level of utilization, tolerating temporary spikes. We recommended setting the soft eviction threshold lower than the hard eviction threshold, but the time period can be operator-specific. The system reservation should also cover the soft eviction threshold.

The soft eviction threshold is an advanced feature. You should configure a hard eviction threshold before attempting to use soft eviction thresholds.
====

In addition, if a soft eviction threshold is met, you can specify a maximum-allowed pod termination grace period to use when evicting pods from the node. If `eviction-max-pod-grace-period` is specified, the node uses the lesser value between the `pod.Spec.TerminationGracePeriodSeconds` and the maximum-allowed grace period. If not specified, the node ends pods immediately with no graceful termination.

For soft eviction thresholds, the following flags are supported:

* `eviction-soft`: a set of eviction thresholds, such as `memory.available<1.5Gi`. If the threshold is met over a corresponding grace period, the threshold triggers a pod eviction.

* `eviction-soft-grace-period`: a set of eviction grace periods, such as `memory.available=1m30s`. The grace period corresponds to how long a soft eviction threshold must hold before triggering a pod eviction.

* `eviction-max-pod-grace-period`: the maximum-allowed grace period (in seconds) to use when terminating pods in response to a soft eviction threshold being met.

To configure soft eviction thresholds, add eviction thresholds to the xref:out-of-resource-eviction-policy[node configuration file] under `eviction-soft`, as shown in xref:out-of-resource-create-config[Configuring Eviction Policies].

.Sample Node Configuration Files with Soft Eviction Thresholds
[source,yaml]
----
kubeletArguments:
  eviction-soft:
  - memory.available<500Mi
  - nodefs.available<500Mi
  - nodefs.inodesFree<5%
  - imagefs.available<100Mi
  - imagefs.inodesFree<10%
  eviction-soft-grace-period:
  - memory.available=1m30s
  - nodefs.available=1m30s
  - nodefs.inodesFree=1m30s
  - imagefs.available=1m30s
  - imagefs.inodesFree=1m30s
----

[NOTE]
====
The values in the sample file are only indicative.
====

[[out-of-resource-allocatable]]
== Configuring the Amount of Resource for Scheduling

To allow the scheduler to fully allocate a node and to prevent evictions, you can control how much of a node resource is made available for scheduling.

Set `system-reserved` equal to the amount of resource that you want available to the scheduler for deploying pods and for system-daemons. The `system-reserved` resources are reserved for operating system daemons, such as *sshd* and *NetworkManager*. Evictions should only occur if pods use more than their requested amount of an allocatable resource.

A node reports two values:

* `Capacity`: How much resource is on the machine.
* `Allocatable`: How much resource is made available for scheduling.

.Procedure

. To configure the amount of allocatable resources, add or modify the `system-reserved` parameter for `eviction-hard` or `eviction-soft` in the appropriate xref:../admin_guide/manage_nodes.adoc#modifying-nodes[node configuration map].
+
[source,yaml]
----
kubeletArguments:
  eviction-hard: <1>
    - "memory.available<500Mi"
  system-reserved:
    - "memory=1.5Gi"
----
<1> This threshold can either be `eviction-hard` or `eviction-soft`.
+
To get appropriate values for the `system-reserved` setting, determine a node's resource usage using the node summary API. For more information, see xref:../admin_guide/allocating_node_resources.adoc#allocating-node-settings[Configuring Nodes for Allocated Resources].

. Restart the {product-title} service for the changes to reflect.
+
ifdef::openshift-enterprise[]
[source,terminal]
----
# systemctl restart atomic-openshift-node
----
endif::[]
ifdef::openshift-origin[]
[source,terminal]
----
# systemctl restart origin-node
----
endif::[]

[[out-of-resource-oscillation-of-node-conditions]]
== Controlling Node Condition Oscillation

If a node oscillates above and below a soft eviction threshold, but does not exceed an associated grace period, the oscillation can cause problems for the scheduler.

To prevent the oscillation, set the `eviction-pressure-transition-period` parameter for a node to control how long it must wait before transitioning out of a pressure condition.

.Procedure

. Add the parameter to the `kubeletArguments` section of the appropriate xref:../admin_guide/manage_nodes.adoc#modifying-nodes[node configuration map] using a set of `<resource_type>=<resource_quantity>` pairs.
+
[source,yaml]
----
kubeletArguments:
  eviction-pressure-transition-period="5m"
----
+
The node toggles the condition back to false when it has not met an eviction threshold for the specified pressure condition during the specified period.
+
[NOTE]
====
Use the default value, 5 minutes, before making any adjustments. The default value is intended to enable the system to stabilize and to prevent the scheduler from scheduling new pods to the node before it has settled.
====

. Restart the {product-title} services for the changes to reflect:
+
ifdef::openshift-enterprise[]
[source,terminal]
----
# systemctl restart atomic-openshift-node
----
endif::[]
ifdef::openshift-origin[]
[source,terminal]
----
# systemctl restart origin-node
----
endif::[]


[[out-of-resource-reclaiming-node-level-resources]]
== Understanding Reclaimation of Node-Level Resources

If an eviction criteria is satisfied, a node initiates the process of reclaiming the pressured resource until the signal is below the defined threshold. During this time, it does not support scheduling of new pods.

A node attempts to reclaim node-level resources before it evicts end-user pods, based on whether the host system has a dedicated `imagefs` configured for the container runtime.

[discrete]
[[reclaiming-with-imagefs]]
===== Host Systems With Imagefs

* If the `nodefs` file system meets eviction thresholds, the node frees disk space by deleting dead pods and containers.

* If the `imagefs` file system meets eviction thresholds, the node frees disk space by deleting all unused images.


[discrete]
[[reclaiming-without-imagefs]]
===== Host Systems Without Imagefs

If the `nodefs` file system meets eviction thresholds, the node frees disk space in the following order:

** Delete dead pods and containers.
** Delete all unused images.

[[out-of-resource-eviction-of-pods]]
== Understanding Pod Eviction

If an eviction threshold is met and the grace period is passed, the node initiates the process of evicting pods until the signal is below the defined threshold.

A node ranks pods for eviction by their xref:../admin_guide/overcommit.adoc#qos-classes[quality of service]. Among pods with the same quality of service, it ranks the pods by the consumption of the compute resource relative to the pod's scheduling request.

Each quality of service level has an out-of-memory score. The Linux out-of-memory tool (OOM killer) uses the score to determine which pods to end. For more information, see xref:out-of-resource-node-out-of-resource-and-out-of-memory[Understanding Quality of Service and Out of Memory Killer].

The following table lists various quality of service levels:

.Quality of Service Levels
[cols="3a,8a",options="header"]
|===

| Quality of Service | Description

|`Guaranteed`
| Pods that consume the highest amount of a resource relative to their request are failed first. If no pod exceeds its request, the strategy targets the largest consumer of the resource.

|`Burstable`
|Pods that consume the highest amount of a resource relative to their request for that resource are failed first. If no pod exceeds its request, the strategy targets the largest consumer of the resource.

|`BestEffort`
| Pods that consume the highest amount of a resource are failed first.
|===

A pod with a guaranteed quality of service is never evicted due to resource consumption by another pod unless either of the following conditions is true:

* A system daemon, such as a node or the container engine, consumes more resources than were reserved using the `system-reserved` allocations.
* A node has only guaranteed quality of service pods remaining.

If a node has only the guaranteed quality of service pods remaining, it evicts a pod that least impacts node stabilityâ€”and limits the impact of the unexpected consumption to the other guaranteed quality of service pods.

Local disk is a best-effort quality of service resource. If necessary, a node evicts pods one at a time to reclaim disk space when experiencing disk pressure.

A node ranks pods by their quality of service:

 * If a node is responding to a lack of free inodes, it reclaims inodes by evicting pods with the lowest quality of service first.
 * If a node is responding to a lack of available disk, it ranks pods within a quality of service that consumes the largest amount of local disk and then evicts those pods first.

[[out-of-resource-node-out-of-resource-and-out-of-memory]]
=== Understanding Quality of Service and Out of Memory Killer


Each container on a node has an `oom_score_adj` value that is based on the quality of service of the pod.

The following table lists quality of service levels and their corresponding `oom_score_adj` values.

.Quality of Service Levels
[cols="3a,8a",options="header"]
|===

| Quality of Service |`oom_score_adj` Value

|`Guaranteed`
| -998

|`Burstable`
| min(max(2, 1000 - (1000 * memoryRequestBytes) / machineMemoryCapacityBytes), 999)

|`BestEffort`
| 1000
|===

If a node is unable to reclaim memory before it experiences a system OOM event, the OOM killer process calculates an OOM score using the `oom_score_adj` value and the percentage of node memory a container is using:

----
oom_score = (% of node memory a container is using) + (oom_score_adj)
----

The node then ends the container with the highest score.

The following containers are ended first:

* Containers with the lowest quality of service.
* Containers that consume the largest amount of memory, relative to the scheduling request.

[NOTE]
====
Unlike pod eviction, if a pod container is ended due to an OOM event, the node can restart the container according to the node restart policy.
====

[[out-of-resource-scheduler]]
== Understanding the Pod Scheduler and OOR Conditions

The pod scheduler (scheduler) checks node conditions before placing additional pods on a node.

Consider a node having an eviction threshold like the following:

----
eviction-hard is "memory.available<500Mi"
----

When the available memory falls below 500Mi, the node reports the `MemoryPressure` value as true in `Node.Status.Conditions`â€”and
the scheduler does not place additional pods on this node.

.Node Conditions and Scheduler Behavior
[cols="3a,8a",options="header"]
|===

|Node Condition |Scheduler Behavior

|`MemoryPressure`
|If a node reports this condition, the scheduler does not place `BestEffort` pods on that node.

|`DiskPressure`
|If a node reports this condition, the scheduler does not place any additional pods on that node.
|===

== Disk Pressure and Supported File System Partitions
When detecting disk pressure, a node supports the `nodefs` and `imagefs` file system partitions.

The `nodefs`, or `rootfs`, is the file system that the node uses for local disk volumes, daemon logs, emptyDir, and other local storage. For example, `rootfs` is the file system that provides *_/_*. The `rootfs` contains `openshift.local.volumes`, by default *_/var/lib/origin/openshift.local.volumes_*.

The `imagefs` is the file system that the container runtime uses for storing images and individual container-writable layers. Eviction thresholds are at 85% full for `imagefs`. The `imagefs` file system depends on the runtime and, in the case of Docker, it is the storage driver that the container uses.

* For Docker:
+
** If you use the `devicemapper` storage driver, the `imagefs` is thin pool.
+
You can limit the read and write layer for the container by setting the `--storage-opt dm.basesize` flag in the Docker daemon.
+
[source,terminal]
----
$ sudo dockerd --storage-opt dm.basesize=50G
----
+
** If you use the `overlay2` storage driver, the `imagefs` is the file system that contains *_/var/lib/docker/overlay2_*.

* For CRI-O, which uses the overlay driver, the `imagefs` is *_/var/lib/containers/storage_* by default.

[NOTE]
====
If you do not use local storage isolation (ephemeral storage) and an XFS quota (volumeConfig), you cannot limit a pod's local disk usage.
====



[[out-of-resource-schedulable-resources-and-eviction-policies]]
== Example Scenario

Consider the following scenario, where you:

* Have a node with a memory capacity of 10Gi.
* Want to reserve 10% of memory capacity for system daemons such as kernel, node, and other daemons.
* Want to evict pods at 95% memory utilization to reduce thrashing and incidence of system OOM.

Based on the configuration mentioned in the scenario, `system-reserved` should include the amount of memory covered by the eviction threshold.

To reach that capacity, either some pod is using more than its request, or the system is using more than 1Gi.

If a node has 10 Gi of capacity and you want to reserve 10% of that capacity for the system daemons with the `system-reserved` setting, perform the following calculation:

----
capacity = 10 Gi
system-reserved = 10 Gi * .1 = 1 Gi
----

The amount of allocatable resources becomes:

----
allocatable = capacity - system-reserved = 9 Gi
----

This means by default, the scheduler will schedule pods that request 9 Gi of
memory to that node.

If you want eviction to be triggered when the node observes that available memory is below 10% of capacity for 30 seconds, or immediately when it falls below 5% of capacity, you need the scheduler to evaluate allocatable as 8Gi. Therefore, ensure your system reservation covers the greater of your eviction thresholds.

----
capacity = 10 Gi
eviction-threshold = 10 Gi * .1 = 1 Gi
system-reserved = (10Gi * .1) + eviction-threshold = 2 Gi
allocatable = capacity - system-reserved = 8 Gi
----

Add the following to the appropriate xref:../admin_guide/manage_nodes.adoc#modifying-nodes[node configuration map]:

[source,yaml]
----
kubeletArguments:
  system-reserved:
  - "memory=2Gi"
  eviction-hard:
  - "memory.available<.5Gi"
  eviction-soft:
  - "memory.available<1Gi"
  eviction-soft-grace-period:
  - "memory.available=30s"
----

This configuration ensures that the scheduler does not place pods on a node and immediately induce memory pressure and trigger an eviction. This configuration assumes that those pods use less than their configured request.

[[out-of-resource-recommended-practices]]
== Recommended Practice

[[out-of-resource-best-practice-daemonset]]
=== Daemon Sets and Out of Resource Handling

A node does not distinguish between a pod created by a daemon set and a pod created by any other object. Consequently, when a node evicts a pod created by a daemon set, the scheduler immediately recreates and reschedules the evicted pod to the same node.

Therefore, in general, daemon sets should not create best effort pods. Ensure that the daemon sets launch pods and configure them with a guaranteed quality of service.
